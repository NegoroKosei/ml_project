##### ^1 ^2
<a name="state-value-opt"></a>
🔖最適状態価値関数
状態 $s$ からスタートし、最適方策に従って行動し続けたときの収益の期待値
例：将棋：ある盤面 $s$ からスタートし、最善手を指し続けたときの勝率

最適行動価値関数
状態sで行動aを取った後、最適方策に従って行動し続けたときの収益の期待値

(4.4) 最適方策は

<a name="state-value-opt2"></a>

---

##### ^3
$p(s'|s,a)$：状態遷移の確率分布 
状態$s$で行動$a$を取ると次の状態$s'$は確率的に決まる（その確率分布が $p(s'|s,a)$）

##### ^4
図4-18 価値関数は方策に依存している（なぜなら価値観数は方策に従って行動し続けたときの収益の期待値だから）
なのでgreedy化で方策を更新していくと価値観数は $V=v_{\mu}$ の直線を離れていく。

##### ^5
式(4.8)🔖
これは一つ手前の価値関数と一つ後の価値関数の間の関係式
行動aによって得られる報酬が $r(s,a,s')$ の項、その後の行動によって得られる収益期待値が $\gamma V(s')$ の項で表されている。