{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83352f5e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 4.3 方策反復法\n",
    "\n",
    "\n",
    "\n",
    "**最終的な目標**: 最適方策 $\\mu_*(s)$ を求めること  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8137dde",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "$\\hspace{10mm} v_*(s) \\hspace{4mm}$    ：最適状態価値関数 [^1🔖]()  \n",
    "$\\hspace{10mm} q_*(s, a)$ ： 最適行動価値関数 [^2🔖]()\n",
    "\n",
    "を用いると、最適方策 $\\mu_*(s)$ は次のように表せる。  \n",
    "\n",
    "$ \\displaystyle \\hspace{10mm} \\begin{alignedat}{2} \n",
    "\\mu_*(s) &= \\underset{a}{\\rm{argmax}} \\hspace{1mm} q_*(s,a) \\hspace{10mm}&(4.4) \\\\\n",
    "         &= \\underset{a}{\\rm{argmax}} \\sum_{s'} p(s'|s, a) \\{r(s,a,s')+\\gamma \\hspace{0.5mm} v_*(s')\\} \\hspace{10mm} &(4.5)  \\end{alignedat} $\n",
    "\n",
    "(4.4)式の通り、最適方策は行動価値関数が最大となる行動 $a$ を選ぶものになっている。  \n",
    "（最も収益の期待値が高くなる行動を選んでいる。）\n",
    "\n",
    "$\\underset{a}{\\rm{argmax}}$ の演算は取れる候補の中から最善の行動を決めている。  \n",
    "このことから 式(4.4), (4.5) から得られる方策は「<span style=\"color:red\">**greedyな方策**</span>」(貪欲な方策)とも呼ばれる。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172415b8",
   "metadata": {},
   "source": [
    "式(4.4) は最適方策 $\\mu_*(s)$ についての式だった。  \n",
    "この式の右辺をある方策 $\\mu$ に適用してみる。\n",
    "\n",
    "$ \\hspace{10mm} \\begin{alignedat}{2} \n",
    "\\mu'(s) &= \\underset{a}{\\rm{argmax}} \\hspace{1mm} q_{\\mu}(s,a) \\hspace{10mm}&(4.6) \\\\\n",
    "         &= \\underset{a}{\\rm{argmax}} \\sum_{s'} p(s'|s, a) \\{r(s,a,s')+\\gamma \\hspace{0.5mm} v_{\\mu}(s')\\} \\hspace{10mm} &(4.7) \\end{alignedat} $\n",
    "$ \\displaystyle \\hspace{20mm} \\left( \\begin{alignedat}{2} \n",
    "q_{\\mu}(s,a) \\text{：方策} \\mu \\text{における行動価値関数} \\\\\n",
    "v_{\\mu}(s) \\text{：方策} \\mu \\text{における状態価値関数}\\end{alignedat}\\right)$\n",
    "\n",
    "**ある方策 $\\mu(s)$ から新たな別の方策 $\\mu'(s)$ を求める式**が得られた。  \n",
    "式(4.6), (4.7) で方策を更新することを「<span style=\"color:red\">**greedy化**</span>」と呼ぶことにしよう。  \n",
    "\n",
    "<div style=\"border: 2px solid #002b47ff; background-color: #002b47ff; padding: 10px; border-radius: 5px;\">\n",
    "<strong>🤔 Q. </strong>\n",
    "<span style=\"color:#ff0400ff\"><strong>greedy化</strong></span>で更新すると方策はどう変化するだろうか？\n",
    "</div>\n",
    "\n",
    "1. **$\\mu$ が最適方策のとき**  \\\n",
    "    $\\mu$ が最適方策のときを考える。このとき、更新式(4.6) の両辺は等しくなる。( ∵式(4.4) )  \n",
    "    ここから最適方策に対してgreedy化を行っても変化しないということがわかる。  \n",
    "    なので**greedy化を行っても変化が小さければ、方策$\\mu$はすでに最適方策に近い**ということになる。\n",
    "\n",
    "2. **greedy化によって方策が大きく更新されるとき**  \n",
    "   greedy化によって大きく更新される場合（更新式(4.6)の$\\mu$と$\\mu'$が異なるとき）はどうなるのだろうか。  \n",
    "   実は常に方策は改善される（最適方策に近づく）ことが証明されている。  \n",
    "   （**方策改善定理**については文献[5]などを参照）\n",
    "\n",
    "\n",
    "<div style=\"border: 2px solid #002b47ff; background-color: #002b47ff; padding: 10px; border-radius: 5px;\">\n",
    "<span style=\"font-size: 14pt\"><u> ✏️ まとめ</u></span>\n",
    "<ul>\n",
    "  <li>方策を<strong>greedy化</strong>することで、<strong>方策は常に改善される</strong></li>\n",
    "  <li>もし方策の改善（更新）がなければ、それが最適方策である</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657d0916",
   "metadata": {},
   "source": [
    "### 4.3.2 評価と改善を繰り返す\n",
    "greedy化する（式(4.7)で更新する）ことで方策は改善できることがわかった。  \n",
    "前回のゼミ、4.2章で価値関数を求める関数を実装した。\n",
    "\n",
    "ここまでで**反復方策法** (Policy Iteration) のキーとなるアイデアが出揃った。**反復方策法**では以下のような流れで処理を行う。\n",
    "\n",
    "1. $\\pi_0$ という方策からスタートする。  \n",
    "   （方策 $\\pi_0$ は確率的方策も考えられるので $\\mu_0(s)$ ではなく $\\pi_0(s|a)$ と表記する）  \n",
    "\n",
    "2. 反復方策評価アルゴリズムで方策 $\\pi_0$ における価値関数 $V_0$ を求める。  \n",
    "\n",
    "3. 価値関数 $v_0$ を使ってgreedy化を行う。（式(4.7)で方策を更新する）  \n",
    "   greedy化された方策は常に一つの行動を選ぶので決定論的な方策 $\\mu_1$ が得られる。\n",
    "\n",
    "4. 2.〜3.を繰り返す。\n",
    "\n",
    "<img src=\"img/fig4_14.jpg\" width=700>\n",
    "\n",
    "最終的にはgreedy化しても方策が更新されなくなり、**最適方策に行き着く**。 \n",
    "  \n",
    "> <span style=\"font-size: 13pt\">✅ 補足</span>  \n",
    "> \n",
    "> 環境は状態遷移確率 $p(s'|s,a)$ と報酬関数 $r(s,a,s')$ によって表される。  \n",
    "> 強化学習の分野では2つをまとめて「環境のモデル」や「モデル」と呼ぶことがある。\n",
    ">\n",
    "> 環境が既知であればエージェントを行動させなくても価値観数を評価できる。  \n",
    "> エージェントを実際に行動させずに最適方策を見つける問題は**プランニング問題**と呼ばれ、この章で扱っているのもプランニング問題。  \n",
    ">  \n",
    "> 実際の強化学習では環境のモデルが既知でない場合が多く、そのときにはエージェントが行動し経験を積みながら最適方策を見つける必要がある。\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb3651f",
   "metadata": {},
   "source": [
    "## 4.4 方策反復法の実装\n",
    "4.2 章と同様に「3x4のグリッドワールド問題」を解く。  \n",
    "（4.2.1の前半で `GridWorld` クラスに実装した）\n",
    "\n",
    "<img src=\"img/fig4_15.jpg\" width=700>\n",
    "\n",
    "方策を評価する（価値観数を求める）関数は4.2.3でもうすでに実装した。  \n",
    "あとはgreedy化で方策を改善する関数を実装すれば方策反復法を使う準備は整う。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a09c0d",
   "metadata": {},
   "source": [
    "### 4.4.1 方策の改善\n",
    "方策を改善するには、以下の式を用いて現状の価値関数に対してgreedyな方策を求めればよい。\n",
    "\n",
    "$ \\hspace{10mm} \\displaystyle\\mu'(s) = \\underset{a}{\\rm{argmax}}\\sum_{s'} p(s'|s,a)\\{r(s',s,a) + \\gamma v{_\\mu}(s')\\} \\hspace{10mm} (4.7) $\n",
    "\n",
    "今回の問題では次の状態は一意に決まる。（ある状態である行動を取ると次の状態 $s'$ は1つに定まる。）  \n",
    "なのでgreedy化の式 (4.7)は次のように簡略化できる。\n",
    "\n",
    "$ \\hspace{10mm} \\displaystyle \\begin{alignedat}{2} & s' = f(s,a) \\text{として} \\\\\n",
    "& \\mu'(s) = \\underset{a}{\\rm{argmax}}\\{r(s',s,a) + \\gamma v_{\\mu}(s')\\} \\hspace{15mm} (4.8)\n",
    "\\end{alignedat} $\n",
    "\n",
    "式(4.8) をもとに **greedyな方策を得る関数** を実装する。  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb8dc96",
   "metadata": {},
   "source": [
    "まずは `argmax` 関数を実装する。  \n",
    "ディクショナリを引数として受け取り、ディクショナリの値が最大となるキーを返す関数になっている。\n",
    "以下のように使えるものを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a863262a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "action_values = {0: 0.1, 1: -0.3, 2: 9.9, 3: -1.3}  # 9.9が最大値（キーは2）\n",
    "max_action = argmax(action_values)\n",
    "print(max_action)   # 出力は2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46882c1",
   "metadata": {},
   "source": [
    "以下が `argmax` 関数の実装である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64501624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(d):\n",
    "    max_value = max(d.values())     # ディクショナリ d の最大値を取得\n",
    "    max_key = 0 \n",
    "    for key, value in d.items():\n",
    "        if value == max_value:      \n",
    "            max_key = key           # 値が最大値に等しいとき、keyの値を返す\n",
    "    return max_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ff907",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "では `argmax` 関数を使って価値観数を greedy化 する関数を実装する。\n",
    "コードは以下の通り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2d137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(V, env, gamma):\n",
    "    pi = {}\n",
    "\n",
    "    # 全ての状態 s について 方策μ(s) を更新する\n",
    "    for state in env.states():\n",
    "        action_values = {}      # key:行動, value:(r+γv) が入るディクショナリ\n",
    "\n",
    "        # 全ての行動の候補について (r+γv) を計算\n",
    "        for action in env.actions():    \n",
    "            next_state = env.next_state(state, action)     # s' = f(s,a) に対応\n",
    "            r = env.reward(state, action, next_state)      # r(s,a,s') に対応\n",
    "            value = r + gamma * V[next_state]               # r+γv を計算\n",
    "            action_values[action] = value\n",
    "\n",
    "        max_action = argmax(action_values)          # (r+γv) が最大となる行動を見つける\n",
    "        action_probes = {0: 0, 1: 0, 2: 0, 3: 0}    # 行動確率\n",
    "        action_probes[max_action] = 1.0             # max_action が必ず選ばれるよう確率を設定\n",
    "        pi[state] = action_probes                   # 方策μ(s)の更新 （状態sでの行動を更新）\n",
    "\n",
    "    return pi   # 更新した方策 μ' を返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3297ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4.4.2 評価と改善を繰り返す\n",
    "4.2で方策を評価する（価値観数を求める）関数を、4.4.1 で方策を改善する関数を実装した。\n",
    "これで評価と改善を繰り返す「**方策反復法**」を実装する準備が整った。\n",
    "\n",
    "**方策反復法を `policy_iter(env, gamma, threshold=0.001, is_render=False)` という関数に実装する。**  \n",
    "\n",
    "引数はそれぞれ以下の通り。\n",
    "- `env` (Environment)： 環境\n",
    "- `gamma` (`float`)： 割引率\n",
    "- `threshold` (`float`)：方策評価を行うとき（価値観数を求めるとき）に更新をストップするしきい値\n",
    "- `is_render` (`bool`)：方策の評価・改善を行う過程を描画するかどうかのフラグ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab002c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def policy_iter(env, gamma, threshold=0.001, is_render=False): \n",
    "    pi = defaultdict(lambda: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25})  # 方策\n",
    "    V = defaultdict(lambda: 0)                                      # 状態価値関数\n",
    "\n",
    "    while True:\n",
    "        V = policy_eval(pi, V, env, gamma, threshold)   # 方策の評価（方策πから価値観数を求める）\n",
    "        new_pi = greedy_policy(V, env, gamma)             # 方策の改善\n",
    "\n",
    "        if is_render:\n",
    "            env.render_v(V, pi)\n",
    "\n",
    "        if new_pi == pi:    # 方策が更新されなくなったら終了\n",
    "            break\n",
    "\n",
    "        pi = new_pi\n",
    "\n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac6c24",
   "metadata": {},
   "source": [
    "`policy_iter` 関数では新旧の方策 $\\pi$ が一致するまで続けているので、最適方策 $\\mu_*(s)$ が得られる。\n",
    "\n",
    "では `policy_iter()` 関数を使って、方策反復法で問題を解いてみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eefa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Gridworld()\n",
    "gamma = 0.9\n",
    "pi = policy_iter(env, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9fa6e5",
   "metadata": {},
   "source": [
    "結果は以下の図のようになる。\n",
    "\n",
    "- 最初はランダムな方策からスタートしており、各マスでの価値関数の値はマイナスになっている。  \n",
    "\n",
    "- 更新を続けると4回目の更新後にはゴール地点以外の全マスでプラスになっている。  \n",
    "\n",
    "- 右図の矢印に注目すると、爆弾を避けてゴールに向かう行動を取っていることがわかる。 \n",
    "\n",
    "<img src=\"img/fig4_16.jpg\" width=\"700\">\n",
    "\n",
    "**方策反復法を使ってこのゲームにおける最適方策が求まった！🎉**\n",
    "\n",
    "> <span style=\"font-size: 13pt\">✅ 補足</span>  \n",
    "> \n",
    "> 3x4のグリッドワールド問題では、最適方策が2種類ある。（左下のマスが↑か➔で2種類）  \n",
    "> 左下のマスからスタートした場合、↑, ➔どちらを選んでも最短でゴールにたどり着ける。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe27c0",
   "metadata": {},
   "source": [
    "## 4.5 価値反復法\n",
    "4.4章では **方策反復法** を使って **最適方策** を求めた。  \n",
    "**方策反復法** では以下の図のように方策の **評価** と **改善** を繰り返していた。  \n",
    "\n",
    "<img src=\"img/fig4_17.jpg\" width=550>\n",
    "\n",
    "2つのプロセスを交互に繰り返すことで最適方策 $\\mu_*$, 最適状態価値関数 $v_*$ へと近づいていった。  \n",
    "その様子は以下のような図にできる。\n",
    "\n",
    "<img src=\"img/fig4_18.jpg\" width=700>\n",
    "\n",
    "**図4-18** では価値関数と方策が取りうる空間を2次元城に図示している。（本当はもっと多次元）  \n",
    "**図4-18** には以下の２つの直線が書き込まれている。  \n",
    "\n",
    "- 直線 $V = v_{\\mu}$ 。任意の価値関数 $V$ と方策 $\\mu$ の真の価値関数 $v_\\mu$ が一致する場所である。  \n",
    "\n",
    "- 直線 $\\mu = \\rm{greedy}(V)$。任意の方策 $\\mu$ と 価値関数 $V$ で greedy化して得られる方策が一致する場所である。\n",
    "\n",
    "評価フェーズ（方策から価値観数を求める過程）は **図4-18 の青矢印** に相当し、  \n",
    "改善フェーズ（greedy化で価値観数から方策を求める過程）は **図4-18 のオレンジ矢印** に相当する。\n",
    "\n",
    "方策評価と方策改善を繰り返して最適解に収束していく過程のことを <u>**一般化方策反復** (Generalized Policy Iteration)</u> を呼ぶ。  \n",
    "多くの強化学習手法はこの一般化方策反復で解釈できる[(1)](https://yagami12.hatenablog.com/entry/2019/02/22/210608#%E4%B8%80%E8%88%AC%E5%8C%96%E6%96%B9%E7%AD%96%E5%8F%8D%E5%BE%A9%EF%BC%88GPI%EF%BC%89)。\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d719126",
   "metadata": {},
   "source": [
    "方策反復法ではゴールにたどり着くまでにジグザグの経路を描く。  \n",
    "ゴールにたどり着くまでの道筋はバリエーションがある。例えば以下の図のような軌道も考えられる。\n",
    "\n",
    "<img src=\"img/fig4_19.jpg\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc2f92",
   "metadata": {},
   "source": [
    "**図4-19** では直線に辿り着く前に折り返している。  \n",
    "実は評価を完全に終える前に改善フェーズに、改善を完全に終える前に評価フェーズに移ることで **図4-19** のような軌道を得ることができる。  \n",
    "\n",
    "> <span style=\"font-size: 13pt\">✅ 補足</span>  \n",
    "> \n",
    "> 方策評価と方策改善を交互に繰り返すアルゴリズムでは、評価と改善の粒度（どのくらい正確に評価、改善を行うか）について自由度がある。  \n",
    "> \n",
    "> 一度の評価, 改善で完全に $\\mu$, $V$ を更新する必要はない。直線に近づいていくだけで良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb971c",
   "metadata": {},
   "source": [
    "4.4章の方策反復法は一般化方策反復のひとつであり、「評価」と「改善」をほぼ完全に行っていた。  \n",
    "\n",
    "\n",
    "<div style=\"border: 2px solid #002b47ff; background-color: #002b47ff; padding: 10px; border-radius: 5px;\">\n",
    "<strong>🤔 Q. </strong>\n",
    "「評価」と「改善」を\"最大限\"行うのではなく、<span style=\"color:#ff0400ff\"><strong>\"最小限\"に</strong></span> 行うとどうなるだろうか？\n",
    "</div>\n",
    "\n",
    "それが <u>**価値反復法** (Value Iteration)</u> のコアにあるアイデアである。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72972d4",
   "metadata": {},
   "source": [
    "### 4.5.1 価値反復法の導出\n",
    "\n",
    "**復習**：方策反復法の評価フェーズでは以下の図のように繰り返し価値観数を更新していた。  \n",
    "    （一回の評価フェーズでn回全マスの価値関数を更新していた。）\n",
    "\n",
    "<img src=\"img/fig4_20.jpg\" width=550>\n",
    "\n",
    "<u>**価値反復法**</u>では評価フェーズで**最小限の更新**しか行わない。  \n",
    "\n",
    "ある1つの状態（1マス）の価値関数を1回更新したらすぐに改善フェーズに移る。  \n",
    "改善フェーズにおいても、1つの状態に対して方策を更新したらすぐ評価フェーズに移る。\n",
    "\n",
    "<img src=\"img/fig4_21.jpg\" width=550>  \n",
    "  \n",
    "$ $\n",
    "\n",
    "> <span style=\"font-size: 13pt\">✅ 補足</span>  \n",
    "> \n",
    "> **図4-21** では改善フェーズから始めているが、別にどちらから始めても構わない\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f185424",
   "metadata": {},
   "source": [
    "では価値反復法を定式化してみよう。\n",
    "\n",
    "#### (1) 改善フェーズの定式化\n",
    "改善フェーズで行う **greedy化** は次のように定式化できた。（式(4.7)と同じ）  \n",
    "\n",
    "$$  $$\n",
    "\n",
    "$ \\displaystyle \\hspace{10mm} \\mu(s) = \\underset{a}{\\rm{argmax}} \\sum_{s'} p(s'|s,a) \\{r(s,a,s')+\\gamma V(s')\\} \\hspace{15mm} (4.8)$  \n",
    "\n",
    "$ \\hspace{20mm} \\left( \\hspace{1mm} V(s) ：現状の価値関数 \\hspace{1mm} \\right)$  \n",
    "\n",
    "改善フェーズでは 式(4.8) のように現在の状態、報酬、次の状態を使い、$\\rm{argmax}$ によって計算する。  \n",
    "\n",
    "$\\rm{argmax}$ によって次の行動は一つに決まるので $mu(s)$ は決定論的方策になっている。\n",
    "\n",
    "<br>\n",
    "\n",
    "#### (2) 評価フェーズの定式化\n",
    "\n",
    "更新前の価値関数を $V(s)$, 更新後の価値観数を $V'(s)$ と表す。  \n",
    "動的計画法(DP)の反復方策評価アルゴリズムでは、以下のように価値関数を更新していたのだった。\n",
    "\n",
    "$ \\displaystyle \\hspace{10mm} V'(s) = \\sum_{a, s'} \\pi(a|s) p(s'|s,a) \\{r(s,a,s')+\\gamma V(s')\\} \\hspace{15mm} (4.9)$   [^5🔖]()\n",
    "\n",
    "式(4.9)では確率論的方策 $\\pi(a|s)$ を使っている。  \n",
    "\n",
    "だが今回はgreedy化によって方策は決定論的（$\\mu(s)$）になっているので、次のように簡略化できる。\n",
    "\n",
    "$ \\displaystyle \\begin{alignedat}{3} \\hspace{10mm} \n",
    "&a = \\mu(s) \\text{として} \\\\\n",
    "&V'(s) = \\sum_{s'} p(s'|s,a) \\{r(s,a,s')+\\gamma V(s')\\} \\hspace{15mm} (4.10) \\end{alignedat}$ \n",
    "\n",
    "評価フェーズではこの更新式(4.10)を使う。\n",
    "\n",
    "---\n",
    "\n",
    "2つの更新式 (4.8) と (4.10) を並べてみる。\n",
    "\n",
    "<img src=\"img/fig4_22.jpg\" width = 650>\n",
    "\n",
    "実は改善フェーズと評価フェーズで同じ計算を行っていることがわかる。\n",
    "2つの式は以下のようにまとめることができる。\n",
    "\n",
    "$ \\displaystyle \\hspace{10mm} V'(s) = \\underset{a}{\\rm{max}}\\sum_{s'} p(s'|s,a) \\{r(s,a,s')+\\gamma V(s')\\} \\hspace{15mm} (4.11)$ \n",
    "\n",
    "最大値を取る $\\rm{max}$ 演算を行って直接価値観数を更新している。**図4-22**の重複していた計算が省かれている。  \n",
    "\n",
    "式 (4.11) で注目するべきなのは、方策 $\\mu$ が登場しないこと。  \n",
    "一切方策を使わず、価値関数のみを更新する。  \n",
    "これが <u>**価値反復法**</u> の名前の由来にもなっている。  \n",
    "\n",
    "価値反復法では一つの更新式 (4.11) のみで「評価」と「改善」を同時に行っている。\n",
    "\n",
    "> <span style=\"font-size: 13pt\">✅ 補足</span>  \n",
    "> \n",
    "> 更新式 (4.11) は ベルマン最適方程式  \n",
    "> \n",
    ">  $ \\displaystyle \\hspace{10mm} v_*(s) = \\underset{a}{\\rm{max}} \\sum_{s'} p(s'|s,a) \\{r(s,a,s')+\\gamma v_*(s')\\} $  \n",
    "> と同じ形である。ベルマン最適方程式を更新式にしたものだとわかる。\n",
    "\n",
    "> <span style=\"font-size: 13pt\">✅ 補足 価値反復法はDPの一種</span>  \n",
    "> \n",
    "> 更新式 (4.11) は次のようにも表せる。  \n",
    ">\n",
    "> $ \\displaystyle \\hspace{10mm} V_{k+1}(s) = \\underset{a}{\\rm{max}}\\sum_{s'} p(s'|s,a) \\{r(s,a,s')+\\gamma V_k(s')\\} $   \n",
    ">\n",
    "> ここでの $V_k(s)$  は $k$ 回更新された価値関数、$V_{k+1}(s)$  は $k+1$ 回更新された価値関数である。\n",
    "> \n",
    "> 価値反復法は $k=0$ から始めて $k=1,2,3\\cdots$ と順に価値関数を更新していくアルゴリズムになっている。  \n",
    "> 動的計画法(DP)の「同じ計算を2度しない」という特徴を満たしているので、DPに属するアルゴリズムである。<p>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "価値反復法で無限回更新すると最適価値関数が得られるが、現実にはどこかで更新を止める必要がある。  \n",
    "更新を止める方法の一つに閾値を使った方法がある。  \n",
    "**閾値をあらかじめ決めておき、全ての状態の更新量が閾値を下回ったときに更新をやめる。  \n",
    "\n",
    "なお更新を繰り返して **最適価値関数 $V_*(s)$** が得られれば、**最適方策** は次のように求まる。（式(4.5)と同じ）\n",
    "\n",
    "$ \\displaystyle \\hspace{10mm} \\mu_*(s) = \\underset{a}{\\rm{argmax}} \\sum_{s'} p(s'|s,a) {r(s,a,s') + \\gamma V_*(s')} \\hspace{15mm} (4.12) $\n",
    "\n",
    "ただ **最適価値関数** から greedy な **方策** を求めているだけである。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33e0015",
   "metadata": {},
   "source": [
    "### 4.5.2 価値反復法の実装\n",
    "\n",
    "「3x4のグリッドワールド問題」の最適方策を価値反復法で求めるプログラムを実装する。\n",
    "この問題では状態遷移は決定論的なので、価値観数の更新式は以下の図のように簡略化できる。\n",
    "\n",
    "<img src=\"img/fig4_23.jpg\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48a3f3",
   "metadata": {},
   "source": [
    "#### (1) 一回の更新を行う関数 `value_iter_onestep()` 関数の実装\n",
    "\n",
    "まずは式(4.13)で（すべての状態について1度だけ）更新を行う関数 `value_iter_onestep()` を実装する。  \n",
    "引数は 価値関数 `V`、 環境 `env`、 割引率 `gamma` 。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab9906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter_onestep(V, env, gamma):\n",
    "    for state in env.states():          # ① 全ての状態についてループ\n",
    "        if state == env.goal_state:     # ゴールの価値関数は常にゼロ\n",
    "            V[state] = 0\n",
    "            continue    # 残りのループ処理はスキップ\n",
    "\n",
    "        action_values = []\n",
    "        for action in env.actions():    # ② 全ての行動について新しい価値関数を計算\n",
    "            next_state = env.next_state(state, action)\n",
    "            r = env.reward(state, action, next_state)\n",
    "            value = r + gamma * V[next_state]   # 新しい価値関数\n",
    "            action_values.append(value)\n",
    "\n",
    "        V[state] = max(action_values)   # 計算した価値関数のうち最大のものを次の価値関数にする\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85104fff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### (2) 最適価値関数を計算する `value_iter()` 関数の実装\n",
    "\n",
    "`value_iter_onestep()`関数 で繰り返し更新し、最適価値関数 $V_*(s)$ を求める `value_iter()` 関数を実装する。  \n",
    "更新が収束するまで `value_iter_onestep()`関数を呼び続けている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3734d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter(V, env, gamma, threshold=0.0001, is_render=True):\n",
    "    while True:\n",
    "        if is_render:\n",
    "            env.render_v(V)     # 状態価値関数の値をヒートマップにプロット\n",
    "        \n",
    "        old_V = V.copy()        # 更新前の価値関数\n",
    "        V = value_iter_onestep(old_V, env, gamma)   # 価値関数の更新\n",
    "\n",
    "        # 更新された量の最大値を求める\n",
    "        delta = 0   # 最大値を記録する変数\n",
    "        for state in V.keys():\n",
    "            t = abs(V[state] - old_V[state])    # 更新量\n",
    "            if delta < t:\n",
    "                delta = t\n",
    "        \n",
    "        # 閾値との比較 （更新量の最大値が閾値を下回ったらやめる）\n",
    "        if delta < threshold:\n",
    "            break\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c29d60",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### (3) `value_iter()` 関数を使って最適方策を求めるプログラム\n",
    "\n",
    "以下の手順で求める\n",
    "\n",
    "1.  `value_iter()` 関数を使って最適価値関数を求める  \n",
    "   \n",
    "2. 最適価値関数からgreedy法で最適方策を求める（以前実装した `greedy_policy()` 関数を使う）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.gridworld import GridWorld\n",
    "from ch04.policy_iter import greedy_policy\n",
    "\n",
    "V = defaultdict(lambda: 0)  # 初期の価値関数 V_0 (要素は全て0で初期化)\n",
    "env = GridWorld()\n",
    "gamma = 0.9         # 割引率の設定\n",
    "\n",
    "V = value_iter(V, env, gamma)       # 価値反復法で最適価値関数を計算\n",
    "\n",
    "pi = greedy_policy(V, env, gamma)   # 最適方策を greedy 法で求める\n",
    "env.render_v(V, pi)                 # 価値関数と方策をヒートマップ上に描画"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76abb294",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### (4) 価値反復法の実行結果\n",
    "\n",
    "(3)のプログラムを実行すると以下の **図4-24* *のような結果が得られる。  \n",
    "\n",
    "価値関数はすべての要素が0の状態から始まり、3回の更新で価値関数が収束していることがわかる。\n",
    "\n",
    "<img src=\"img/fig4_24.jpg\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d3ec8",
   "metadata": {},
   "source": [
    "また、greedy法で求めた最適方策も合わせてプロットしたのが以下の **図4-25** である。\n",
    "\n",
    "方策反復法(P116)のときと同じ最適価値関数と最適方策が求まっている！🎉 \n",
    "\n",
    "方策反復法では4回の更新を行っていたので、より効率的に最適方策を求められた👍\n",
    "\n",
    "<img src=\"img/fig4_25.jpg\" width=600>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e5a7af",
   "metadata": {},
   "source": [
    "## 4.6 まとめ\n",
    "\n",
    "この章では動的計画法 (DP) を使って **最適方策*** を求める方法を学んだ。\n",
    "具体的には以下の2つのアルゴリズムで求めた。\n",
    "\n",
    "| アルゴリズム | 内容 | \n",
    "| :-- | :-- |\n",
    "| **方策反復法** | 方策の「評価」と「改善」を繰り返す。評価では価値関数をDPを評価し、改善では価値関数からgreedy法で方策を求める。|\n",
    "| **価値反復法** | 評価と改善を融合している。価値関数を繰り返し更新し最適価値関数を求める。最適方策は最適価値関数からgreedy法で求める。 | \n",
    "\n",
    "実際に2つのアルゴリズムを「グリッドワールド問題」に適用し、**最適方策** を得ることができた！🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-projects (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
